# -*- coding: utf-8 -*-
"""Analysis_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/106Fe0IRWG7BEmuU--ozeMUpV2qqUjtVE
"""

system_message = """
You are a highly intelligent and professional AI-powered financial assistant bot designed to help users understand and manage their personal finances based on spending data they provide. Your core responsibilities include analyzing financial behavior, identifying spending patterns, flagging potential issues, and offering smart, tailored advice.

When a user submits transaction or spending data — either as a list, table, or natural language description — respond with clear, well-reasoned insights and practical suggestions. You may be asked questions like:

“How am I spending across different categories?”

“What can I do to save more money?”

“Are there any unusual or excessive expenses?”

“Can you give me a monthly summary?”

“How can I reduce my grocery or travel spending?”

Your responses should always be:

Markdown-formatted and professionally structured.

Friendly but formal in tone.

Data-driven, with specific observations and suggestions based on the input.

Adaptable to incomplete data — make reasonable assumptions and clearly state them if needed.

If the data lacks categorization or dates, try your best to infer them logically. Always aim to be helpful and insightful, like a virtual Chartered Accountant or financial advisor.
"""

import json
import re
from pathlib import Path
import google.generativeai as genai

# Replace with your actual API key and file path
  # Replace with your actual key
file_path = "/content/drive/MyDrive/Document1.pdf"

# Configure the API key
genai.configure(api_key="AIzaSyCS0YUu2WtoHbq24eJ3YeEkdDm3n0mGNbk")

# Initialize the Gemini model
model = "models/gemini-1.5-flash"

# Define the prompt
prompt = """
Analyze the attached PDF document and extract all expense-related data in a structured format. Each expense entry should include the following fields:

- Date: The transaction or expense date, in YYYY-MM-DD format if available.
- Description: A brief summary of the transaction or line item.
- Amount: The expense amount, formatted as a number (e.g., 125.50).
- Quantity: Number of items
-
- Category: Classify the expense into one of the following categories, if possible:
  - Electronics
  - Groceries
  - Hardware
  - House Ware

Instructions:
- Return the output as a **raw JSON array** (no markdown formatting, no ```json blocks).
- Include all relevant expenses found in the document, even across multiple pages or sections.
- If a field is missing or unclear, include it with a null value or "unknown" as appropriate.
- Ignore non-expense information such as cover pages, headers, footers, or unrelated notes.
"""

# Load the PDF file as bytes
pdf_bytes = Path(file_path).read_bytes()

# Prepare content for the Gemini model
contents = [
    {"text": prompt},
    {
        "inline_data": {
            "mime_type": "application/pdf",
            "data": pdf_bytes
        }
    }
]

# Generate response
try:
    response = genai.GenerativeModel(model).generate_content(contents=contents)
    raw_text = response.text.strip()

    # Remove markdown-style ```json ... ``` if present
    if raw_text.startswith("```json") or raw_text.startswith("```"):
        raw_text = re.sub(r"^```(?:json)?\s*", "", raw_text)
        raw_text = re.sub(r"\s*```$", "", raw_text)

    # Attempt to parse JSON
    try:
        parsed = json.loads(raw_text)
        print("\nParsed JSON:\n", json.dumps(parsed, indent=2))
    except json.JSONDecodeError as e:
        print("\nWarning: The response is not valid JSON.")
        print("Error:", str(e))
        # print("Raw Output:\n", raw_text)

except Exception as e:
    raise Exception(f" Error from Gemini API: {e}")

text = []
for i in range(len(parsed)):
    text.append(f"Description: {parsed[i]['Description']}, Quantity: {parsed[i]['Quantity']}, Amount: {parsed[i]['Amount']}, Category: {parsed[i]['Category']}")

text

from langchain_core.documents import Document
docs = []
for i in text:
    docs.append(Document(i))
def convert_to_natural_language(doc):
    text = doc.page_content
    try:
        parts = dict(part.strip().split(": ") for part in text.split(", "))
        return f"{parts['Quantity']} {parts['Description']} costs ₹{parts['Amount']} and it falls under the {parts['Category']} category."
    except:
        return text

converted_docs = [Document(page_content=convert_to_natural_language(doc)) for doc in docs]
converted_docs

!pip install faiss-cpu

!pip install langchain-community

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
vectorstore = FAISS.from_documents(converted_docs, embedding_model)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 10})

from google.colab import userdata
google_api = userdata.get('GeminiAPI')

!pip install langchain_google_genai

from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.7,
    google_api_key = google_api
)

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder
from langchain_community.chat_models import ChatOllama
from langchain_core.documents import Document
from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.messages import HumanMessage
prompt = ChatPromptTemplate.from_messages([
    ("system", system_message),
    ("human", "Question: {question}\n\nRetrieved context:\n{context}")
])

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    google_api_key = google_api
)

memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)

rag_chain = (
    {"question": RunnablePassthrough(),
     "context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs))}
    | prompt
    | llm
)

print(rag_chain.invoke("How much am I spending on electronincs ?"))

rag_chain.invoke("do I have the ingredients to make carrot halwa ?")

